## Project FAQ: Understanding the Directive-Driven AI System

This section addresses common questions about the project's methodology, goals, and technical implementation.

Q1: What problem does this project address?

A1: Current Large Language Models (LLMs) like GPT series, while powerful, are inherently unpredictable and opaque. They can hallucinate, drift from instructions, contradict themselves, and lack built-in mechanisms for verifiable reliability or transparency in their reasoning. This limits their trustworthiness and applicability in critical domains.

Q2: How does the project propose to make LLMs more reliable and transparent?

A2: The project introduces an external system architecture, referred to as the 'Guardian', that works with the LLM. It uses a structured, human-readable 'directive scaffold'—a set of defined rules—to guide and constrain LLM behaviour. Crucially, the Guardian system validates the LLM's output against these rules, providing a layer of verifiable enforcement external to the LLM itself.

Q3: How does Canela help guard against 'AI Slop' (aka 'The dead internet problem')?

A3: Without the rule-set, Guardian would happily pass through any text and merely stamp a hash.
Directives force the model to expose premise / source / confidence so that low-effort slop fails the validator.

Q4: What is the 'directive scaffold'?

A4: The directive scaffold is a defined set of natural-language rules covering various aspects of AI behaviour, such as memory integrity, logical consistency, disclosure of uncertainty (epistemic humility), avoidance of misleading statements, and adherence to specified output formats. It's designed to be human-readable and auditable.

Q5: How does the 'Guardian' system work in practice?

A5: The Guardian is a software component (or middleware) that sits between the user/application and the LLM. It loads the verified directive set (confirming its integrity via blockchain). When a user provides input, the Guardian constructs a prompt that includes the relevant directives to bias the LLM's response. After the LLM generates output, the Guardian automatically validates this output against the rules in the directive scaffold using programmed checks. If the output is non-compliant, the Guardian can flag it, intercept it, or request the LLM to regenerate the response.

Q6: If LLMs are just probabilistic text prediction engines, how can they possibly follow complex rules like 'logical extension' or 'falsification' contained in the directives?

A6: LLMs do not "understand" or "execute" rules programmatically in the human sense. Their adherence is based on statistically predicting compliant output given the prompt context (including the directives and instructions to follow them). The critical part is the Guardian's validation logic. The Guardian is programmed to check the LLM's observable output for patterns or indicators that demonstrate adherence to or violation of a rule. For example, for logical rules, the Guardian might check for contradictions in the output or look for structural signs of step-by-step reasoning, rather than performing the reasoning itself. Its 'understanding' is operational and programmatic, defined by the validation code, not by general intelligence.

Q7: Does the 'Guardian' need to be an AI itself, perhaps more powerful than the LLM it oversees?

A7: No, the Guardian does not need to be an AI or have general intelligence. It is a standard software system programmed with explicit, deterministic logic for validation and control. Its capabilities are narrow and defined by human-written code, not learned from data like an LLM. Its 'intelligence' is in the design of the validation rules and the logic to enforce them, not in generative or cognitive abilities.

Q8: Why use an LLM at all if a separate 'Guardian' system is needed to ensure reliability?

A8: The LLM is still essential because it is the powerful generative engine and vast knowledge source. It can understand natural language input, access patterns from massive datasets, and generate coherent, complex, and creative text. The Guardian cannot do this. The LLM produces the raw output; the Guardian provides the structure, integrity, and verifiable control around that probabilistic generation. You leverage the LLM's strengths while mitigating its weaknesses with external engineering.

Q9: How does blockchain fit into this methodology?

A9: Blockchain is used for its core properties of immutability and transparency, not for cryptocurrency or typical Web3 applications. The cryptographic hash (e.g., SHA-256) of the directive scaffold is anchored on a public blockchain. This provides a tamper-evident record, allowing anyone to verify that the rule-set used by the Guardian is the authentic and unaltered version. It adds a layer of trust and auditability to the governance process itself.

Q10: Is this approach technically feasible with current technology?

A10: Yes, the proposed system architecture—combining existing LLM APIs, standard software engineering for the Guardian middleware (including prompt construction, validation logic, API communication), and interaction with public blockchains for anchoring hashes—is entirely within the scope of current technological capabilities. It does not require breakthroughs in core LLM architecture.

Q11: Where is the empirical testing data for this methodology?

A11: The project is currently at the conceptual framework and architectural design stage. While preliminary observations were made using directives with LLMs (suggesting potential benefits like reduced inconsistency), rigorous, systematic empirical validation requires the 'Guardian' infrastructure to be fully built and implemented to provide a controlled testing environment. Obtaining reliable data is the crucial next phase of the project.

Q12: The project is currently conceptual. How can others get involved or learn more?

A12: The project is open and seeking collaboration to build the 'Guardian' infrastructure and conduct rigorous testing. The conceptual framework, directive schema, and plan for validation are publicly available. You can find more details, including documentation, schema definitions, and potential code contributions, on the project's public GitHub repository at [Link to GitHub Repo]. A more formal technical brief and related documents are available on OSF at [Link to OSF Project Page].

Q13: What is the correct terminology for the 'Guardian' layer?

A13: While terms like 'layer' or 'framework' are acceptable, more technically precise terms for the Guardian's function and position include middleware, validation layer, control layer, or orchestration layer. Referring to it as a "software abstraction layer" is less accurate as its purpose is to add concrete structure and constraints, not to hide complexity. Using "the Guardian system" or "the Guardian framework" is also clear and appropriate.

Q14: What about ethical guidelines or harmful content? Are these covered by the directives?

A14: Yes, the directive scaffold is intended to include ethical principles and rules aimed at preventing the generation of harmful, biased, or misleading content. The Guardian's validation logic would be designed to check for violations of these specific directives, similar to how it checks for logical consistency or hallucination. This provides a transparent and auditable layer for enforcing ethical guidelines defined by the project.

Q15: How does this project compare to existing efforts in LLM alignment or safety?

A15: Many existing efforts focus on aligning LLMs through further training (fine-tuning) or basic prompt engineering. While valuable, these methods can still be opaque and subject to model drift. This project complements those efforts by proposing an *external, verifiable system* that enforces a defined rule-set independent of the LLM's internal weights or training data. The use of blockchain anchoring for the rule-set's integrity adds a unique layer of transparency and auditability to the governance process itself.

Q16: Isn’t CANDELA just “chain-of-thought” prompting with a new name?

A16: Traditional chain-of-thought simply places reasoning steps inside each prompt; the model may or may not follow them, and there is no lasting record of what rules applied. CANDELA stores those steps once as micro-directives in a hashed JSON file, anchors that hash on a public blockchain, and runs an automatic validator after every reply. This means anyone can verify (a) which rules were in force before generation and (b) whether the model actually followed them, providing auditability and safety that plain chain-of-thought lacks.

Q17: What are the next steps for the project?

A17: The immediate next steps involve building the 'Guardian' software prototype, implementing the validation logic based on the directive schema, setting up the blockchain anchoring mechanism, and using this infrastructure to conduct rigorous empirical testing of the methodology's effectiveness in controlling LLM behaviour and improving reliability. Concurrently, we aim to disseminate the project's details through academic channels (e.g., arXiv preprints, workshop papers) and continue seeking collaboration.

Q18: ### 10. How can I get involved?

A18:

1. ⭐ Star the repo to follow updates.  
2. File issues for bugs or directive suggestions.  
3. Submit Pull Requests for code, tests, or documentation.  
4. Join the upcoming Discord (link in README) for discussions.

